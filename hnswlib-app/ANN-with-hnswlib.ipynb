{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","#### Intro\n","This example uses Approximate Nearest Neighbor Search (ANN) with Hnswlib  (https://github.com/nmslib/hnswlib/).\n","\n","Install it with `pip install hnswlib`\n","\n","For an embeddings model, we use the SBERT model 'msmarco-distilbert-base-v4'\n","\n","For a dataset we use our own (at ../dish-washer-data.csv); that gives us a way to compare with naive search\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/auro/anaconda3/envs/openai/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import csv\n","import pickle\n","import time\n","import hnswlib\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer, util"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model_name = 'msmarco-distilbert-base-v4'\n","embeddings_model = SentenceTransformer(model_name)\n","embedding_size = 768    #Size of embeddings\n","top_k_hits = 5         #Output k hits\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    embeddings = model.encode([text])\n","    return embeddings\n","\n","def get_doc_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    return get_embedding(text, model)\n","\n","def get_query_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    return get_embedding(text, model)\n","\n","def compute_doc_embeddings(df: pd.DataFrame, model: SentenceTransformer) -> dict[tuple[str, str], list[float]]:\n","    \"\"\"\n","    Create an embedding for each row in the dataframe.\n","    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n","    \"\"\"\n","    return {\n","        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \"), model) for idx, r in df.iterrows()\n","    }"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-computed embeddings from disc...\n","Loading done.\n","Length, corpus sentences:149\n","Length, corpus embeddings:149\n","one vector dimension: 768\n","Sentence samples\n","                                                              content  tokens\n","title     heading                                                           \n","Chapter_2 112       DOOR DETERGENT DISPENSERS NOT WORKING: If the...      83\n","Chapter_5 250       TECH NOTES: Late-model Whirlpool (sidewinders...      98\n","Chapter_6 352       However, if you're a DIY'er, it's a BIG probl...     105\n","Chapter_1 25        What is absolutely the worst enemy of electri...     151\n","Chapter_6 380       Most components are tested simply by removing...      73\n"]}],"source":["embeddings_cache_path = f\"dishwasher-repair-manual-embeddings-{model_name.replace('/', '_')}.pkl\"\n","# print(embeddings_cache_path)\n","\n","#Check if embeddings cache path exists\n","if not os.path.exists(embeddings_cache_path):\n","    df = pd.read_csv('../dish-washer-data.csv')\n","    df[\"tokens\"] = pd.to_numeric(df[\"tokens\"])  # convert column \"tokens\" of a DataFrame\n","    df = df.set_index([\"title\", \"heading\"])\n","    print(f\"{len(df)} rows in the data.\")\n","    print(df.sample(10))\n","    # TODO get some stats on max/min content length\n","    corpus_size = df.shape[0]\n","\n","    # This could take a bit of time\n","    print(\"Encoding the corpus. This might take a while...\")\n","    corpus_embeddings = compute_doc_embeddings(df, embeddings_model)\n","\n","    print(\"Store file...\")\n","    with open(embeddings_cache_path, \"wb\") as fOut:\n","        pickle.dump({'sentences': df, 'embeddings': corpus_embeddings}, fOut)\n","else:\n","    print(\"Loading pre-computed embeddings from disc...\")\n","    with open(embeddings_cache_path, \"rb\") as f_in:\n","        cache_data = pickle.load(f_in)\n","        corpus_sentences = cache_data['sentences']\n","        corpus_embeddings = cache_data['embeddings']\n","        corpus_embeddings = [v for k, v in corpus_embeddings.items()]\n","        print(\"Loading done.\")\n","        print(f'Length, corpus sentences:{len(corpus_sentences)}')\n","        print(f'Length, corpus embeddings:{len(corpus_embeddings)}')\n","        print(f'one vector dimension: {len(corpus_embeddings[0][0])}')\n","\n","        print(f\"Sentence samples\\n {corpus_sentences.sample(5)}\")\n","        \n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading index...\n","Loading index done.\n","MultiIndex([('Chapter_1',   0),\n","            ('Chapter_1',   2),\n","            ('Chapter_1',   4),\n","            ('Chapter_1',  21),\n","            ('Chapter_1',  24),\n","            ('Chapter_1',  25),\n","            ('Chapter_1',  29),\n","            ('Chapter_1',  32),\n","            ('Chapter_1',  38),\n","            ('Chapter_1',  42),\n","            ...\n","            ('Chapter_6', 419),\n","            ('Chapter_6', 421),\n","            ('Chapter_6', 424),\n","            ('Chapter_6', 426),\n","            ('Chapter_6', 429),\n","            ('Chapter_6', 431),\n","            ('Chapter_6', 432),\n","            ('Chapter_6', 434),\n","            ('Chapter_6', 436),\n","            ('Chapter_6', 438)],\n","           names=['title', 'heading'], length=149)\n"]}],"source":["#Defining our hnswlib index\n","index_path = \"./hnswlib.index\"\n","import hnswlib\n","import numpy as np\n","index = hnswlib.Index(space = 'cosine', dim = embedding_size)\n","\n","if os.path.exists(index_path):\n","    print(\"Loading index...\")\n","    index.load_index(index_path)\n","    print('Loading index done.')\n","else:\n","    ### Create the HNSWLIB index\n","    print(\"Start creating HNSWLIB index\")\n","    # TODO check is ef_consturction and M and appropriate for this dataset\n","    index.init_index(max_elements = len(corpus_embeddings), ef_construction = 400, M = 64)\n","\n","    print(list(range(len(corpus_embeddings))))\n","    # Then we train the index to find a suitable clustering\n","    corpus_embeddings = np.array(corpus_embeddings).squeeze()\n","    print('shape', corpus_embeddings.shape)\n","    # index.add_items(corpus_embeddings, list(range(len(corpus_embeddings))))\n","    index.add_items(corpus_embeddings, list(corpus_sentences.index))\n","\n","    print(\"Saving index to:\", index_path)\n","    index.save_index(index_path)\n","\n","# Controlling the recall by setting ef:\n","index.set_ef(50)  # ef should always be > top_k_hits\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["inp_question = \"Why is my dishwasher leaking?\""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["length of question embedding 768\n","hits [{'corpus_id': 55, 'score': 0.7006855607032776}, {'corpus_id': 20, 'score': 0.5618450045585632}, {'corpus_id': 56, 'score': 0.5229777693748474}, {'corpus_id': 40, 'score': 0.507260799407959}, {'corpus_id': 1, 'score': 0.505429208278656}]\n","Input question: Why is my dishwasher leaking?\n","Results (after 0.021 seconds):\n","type <class 'pandas.core.frame.DataFrame'>\n","\t0.701\tcontent     There are two general areas where you will co...\n","tokens                                                    77\n","Name: (Chapter_4, 190), dtype: object\n","\t0.562\tcontent     POOR WASH QUALITY: This is the most common co...\n","tokens                                                    77\n","Name: (Chapter_2, 81), dtype: object\n","\t0.523\tcontent     A slow, under-tub leak may go years without b...\n","tokens                                                    71\n","Name: (Chapter_4, 191), dtype: object\n","\t0.507\tcontent     NOTE: This Chapter assumes that the motor is ...\n","tokens                                                   124\n","Name: (Chapter_3, 145), dtype: object\n","\t0.505\tcontent     The main reason dishwashers exist is that the...\n","tokens                                                    76\n","Name: (Chapter_1, 2), dtype: object\n"]}],"source":["start_time = time.time()\n","question_embedding = embeddings_model.encode(inp_question)\n","print('length of question embedding', len(question_embedding))\n","\n","#We use hnswlib knn_query method to find the top_k_hits\n","corpus_ids, distances = index.knn_query(question_embedding, k=top_k_hits)\n","# print('corpus_ids', corpus_ids)\n","\n","# We extract corpus ids and scores for the query\n","hits = [{'corpus_id': id, 'score': 1 - score} for id, score in zip(corpus_ids[0], distances[0])]\n","hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n","print('hits', hits)\n","end_time = time.time()\n","\n","print(\"Input question:\", inp_question)\n","print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n","print('type', type(corpus_sentences))\n","for hit in hits[0:top_k_hits]:\n","    print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences.iloc[hit['corpus_id']]))\n","\n","# # Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n","# # Here, we compute the recall of ANN compared to the exact results\n","# correct_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k_hits)[0]\n","# correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n","\n","# ann_corpus_ids = set([hit['corpus_id'] for hit in hits])\n","# if len(ann_corpus_ids) != len(correct_hits_ids):\n","#     print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n","\n","# recall = len(ann_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n","# print(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n","\n","# if recall < 1:\n","#     print(\"Missing results:\")\n","#     for hit in correct_hits[0:top_k_hits]:\n","#         if hit['corpus_id'] not in ann_corpus_ids:\n","#             print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n","# print(\"\\n\\n========\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('openai')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3c5ba64e8a2a59a6be0dd5b8a0149005a96dade111bc9e6b7c5d65266a44d405"}}},"nbformat":4,"nbformat_minor":2}
