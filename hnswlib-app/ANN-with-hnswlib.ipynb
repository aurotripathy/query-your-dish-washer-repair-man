{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","# #Defining our hnswlib index\n","# index_path = \"./hnswlib.index\"\n","# #We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n","# index = hnswlib.Index(space = 'cosine', dim = embedding_size)\n","\n","# if os.path.exists(index_path):\n","#     print(\"Loading index...\")\n","#     index.load_index(index_path)\n","# else:\n","#     ### Create the HNSWLIB index\n","#     print(\"Start creating HNSWLIB index\")\n","#     index.init_index(max_elements = len(corpus_embeddings), ef_construction = 400, M = 64)\n","\n","#     # Then we train the index to find a suitable clustering\n","#     index.add_items(corpus_embeddings, list(range(len(corpus_embeddings))))\n","\n","#     print(\"Saving index to:\", index_path)\n","#     index.save_index(index_path)\n","\n","# # Controlling the recall by setting ef:\n","# index.set_ef(50)  # ef should always be > top_k_hits\n","\n","# ######### Search in the index ###########\n","\n","# print(\"Corpus loaded with {} sentences / embeddings\".format(len(corpus_sentences)))\n","\n","# while True:\n","#     inp_question = input(\"Please enter a question: \")\n","\n","#     start_time = time.time()\n","#     question_embedding = model.encode(inp_question)\n","\n","#     #We use hnswlib knn_query method to find the top_k_hits\n","#     corpus_ids, distances = index.knn_query(question_embedding, k=top_k_hits)\n","\n","#     # We extract corpus ids and scores for the first query\n","#     hits = [{'corpus_id': id, 'score': 1-score} for id, score in zip(corpus_ids[0], distances[0])]\n","#     hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n","#     end_time = time.time()\n","\n","#     print(\"Input question:\", inp_question)\n","#     print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n","#     for hit in hits[0:top_k_hits]:\n","#         print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n","\n","#     # Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n","#     # Here, we compute the recall of ANN compared to the exact results\n","#     correct_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k_hits)[0]\n","#     correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n","\n","#     ann_corpus_ids = set([hit['corpus_id'] for hit in hits])\n","#     if len(ann_corpus_ids) != len(correct_hits_ids):\n","#         print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n","\n","#     recall = len(ann_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n","#     print(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n","\n","#     if recall < 1:\n","#         print(\"Missing results:\")\n","#         for hit in correct_hits[0:top_k_hits]:\n","#             if hit['corpus_id'] not in ann_corpus_ids:\n","#                 print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n","#     print(\"\\n\\n========\\n\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\nThis example uses Approximate Nearest Neighbor Search (ANN) with Hnswlib  (https://github.com/nmslib/hnswlib/).\\nInstall it with `pip install hnswlib`\\n\\nFor an embeddings model, we use the SBERT model 'msmarco-distilbert-base-v4'\\nFor a dataset we use our own; gives us a way to compare with naive search\\n\""]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","This example uses Approximate Nearest Neighbor Search (ANN) with Hnswlib  (https://github.com/nmslib/hnswlib/).\n","Install it with `pip install hnswlib`\n","\n","For an embeddings model, we use the SBERT model 'msmarco-distilbert-base-v4'\n","For a dataset we use our own; gives us a way to compare with naive search\n","\"\"\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/auro/anaconda3/envs/openai/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from sentence_transformers import SentenceTransformer, util\n","import os\n","import csv\n","import pickle\n","import time\n","import hnswlib\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model_name = 'msmarco-distilbert-base-v4'\n","embeddings_model = SentenceTransformer(model_name)\n","embedding_size = 768    #Size of embeddings\n","top_k_hits = 5         #Output k hits\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dishwasher-repair-manual-embeddings-msmarco-distilbert-base-v4.pkl\n","Loading pre-computed embeddings from disc\n","Loafing done\n"]}],"source":["embeddings_cache_path = f\"dishwasher-repair-manual-embeddings-{model_name.replace('/', '_')}.pkl\"\n","print(embeddings_cache_path)\n","\n","#Check if embeddings cache path exists\n","if not os.path.exists(embeddings_cache_path):\n","    # Check if the dataset exists. If not, read them\n","    df = pd.read_csv('../dish-washer-data.csv')\n","    df[\"tokens\"] = pd.to_numeric(df[\"tokens\"])  # convert column \"tokens\" of a DataFrame\n","    df = df.set_index([\"title\", \"heading\"])\n","    print(f\"{len(df)} rows in the data.\")\n","    df.sample(10)\n","    # TODO get some stats on max/min content length\n","    corpus_size = df.shape[0]\n","\n","    print(\"Encode the corpus. This might take a while...\")\n","    # This could take a bit of time\n","    # document_embeddings = compute_doc_embeddings(df, embeddings_model)\n","    corpus_embeddings = embeddings_model.encode(list(df), show_progress_bar=True, convert_to_numpy=True)\n","\n","    print(\"Store file on disc\")\n","    with open(embeddings_cache_path, \"wb\") as fOut:\n","        pickle.dump({'sentences': list(df), 'embeddings': corpus_embeddings}, fOut)\n","else:\n","    print(\"Loading pre-computed embeddings from disc\")\n","    with open(embeddings_cache_path, \"rb\") as fIn:\n","        cache_data = pickle.load(fIn)\n","        corpus_sentences = cache_data['sentences']\n","        corpus_embeddings = cache_data['embeddings']\n","        print(\"Loafing done\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('openai')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3c5ba64e8a2a59a6be0dd5b8a0149005a96dade111bc9e6b7c5d65266a44d405"}}},"nbformat":4,"nbformat_minor":2}
