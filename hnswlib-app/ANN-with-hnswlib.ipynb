{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\nThis example uses Approximate Nearest Neighbor Search (ANN) with Hnswlib  (https://github.com/nmslib/hnswlib/).\\nInstall it with `pip install hnswlib`\\n\\nFor an embeddings model, we use the SBERT model 'msmarco-distilbert-base-v4'\\nFor a dataset we use our own; gives us a way to compare with naive search\\n\""]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","This example uses Approximate Nearest Neighbor Search (ANN) with Hnswlib  (https://github.com/nmslib/hnswlib/).\n","Install it with `pip install hnswlib`\n","\n","For an embeddings model, we use the SBERT model 'msmarco-distilbert-base-v4'\n","For a dataset we use our own; gives us a way to compare with naive search\n","\"\"\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/auro/anaconda3/envs/openai/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import csv\n","import pickle\n","import time\n","import hnswlib\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer, util"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model_name = 'msmarco-distilbert-base-v4'\n","embeddings_model = SentenceTransformer(model_name)\n","embedding_size = 768    #Size of embeddings\n","top_k_hits = 5         #Output k hits\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    embeddings = model.encode([text])\n","    return embeddings\n","\n","def get_doc_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    return get_embedding(text, model)\n","\n","def get_query_embedding(text: str, model: SentenceTransformer) -> list[float]:\n","    return get_embedding(text, model)\n","\n","def compute_doc_embeddings(df: pd.DataFrame, model: SentenceTransformer) -> dict[tuple[str, str], list[float]]:\n","    \"\"\"\n","    Create an embedding for each row in the dataframe.\n","    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n","    \"\"\"\n","    return {\n","        idx: get_doc_embedding(r.content.replace(\"\\n\", \" \"), model) for idx, r in df.iterrows()\n","    }"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-computed embeddings from disc...\n","Loading done.\n","Length, corpus sentences:149\n","Length, corpus embeddings:149\n","type corpus embeddings: <class 'list'>\n","one vector dimension: 768\n","sentence samples\n","                                                             content  tokens\n","title     heading                                                           \n","Chapter_3 183       Cutlery with wood, bone or horn handles may c...      74\n","Chapter_4 209       The solution is to shut off the water supply ...      73\n","Chapter_2 98        Disassemble or remove the pump and motor unit...      91\n","Chapter_6 365       Remember that for something to be energized, ...      88\n","Chapter_4 199       To get through the cycle you've already start...      74\n","          216       In sidewinder machines with butterfly drain v...      78\n","Chapter_6 408       A motor that is trying to start, but can't fo...      85\n","Chapter_2 104       Take the pump housing apart as described in C...      81\n","Chapter_5 260       You usually need to remove the machine from b...     113\n","          273       Feel the action of the flapper valve. If it i...      96\n"]}],"source":["embeddings_cache_path = f\"dishwasher-repair-manual-embeddings-{model_name.replace('/', '_')}.pkl\"\n","# print(embeddings_cache_path)\n","\n","#Check if embeddings cache path exists\n","if not os.path.exists(embeddings_cache_path):\n","    # Check if the dataset exists. If not, read them\n","    df = pd.read_csv('../dish-washer-data.csv')\n","    df[\"tokens\"] = pd.to_numeric(df[\"tokens\"])  # convert column \"tokens\" of a DataFrame\n","    df = df.set_index([\"title\", \"heading\"])\n","    print(f\"{len(df)} rows in the data.\")\n","    print(df.sample(10))\n","    # TODO get some stats on max/min content length\n","    corpus_size = df.shape[0]\n","\n","    # This could take a bit of time\n","    print(\"Encoding the corpus. This might take a while...\")\n","    corpus_embeddings = compute_doc_embeddings(df, embeddings_model)\n","\n","    print(\"Store file...\")\n","    with open(embeddings_cache_path, \"wb\") as fOut:\n","        pickle.dump({'sentences': df, 'embeddings': corpus_embeddings}, fOut)\n","else:\n","    print(\"Loading pre-computed embeddings from disc...\")\n","    with open(embeddings_cache_path, \"rb\") as f_in:\n","        cache_data = pickle.load(f_in)\n","        corpus_sentences = cache_data['sentences']\n","        corpus_embeddings = cache_data['embeddings']\n","        corpus_embeddings = [v for k, v in corpus_embeddings.items()]\n","        print(\"Loading done.\")\n","        print(f'Length, corpus sentences:{len(corpus_sentences)}')\n","        print(f'Length, corpus embeddings:{len(corpus_embeddings)}')\n","        print(f'type corpus embeddings: {type(corpus_embeddings)}')\n","        print(f'one vector dimension: {len(corpus_embeddings[0][0])}')\n","\n","        print(f\"sentence samples\")\n","        print(corpus_sentences.sample(10))\n","        \n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Start creating HNSWLIB index\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148]\n","shape (149, 768)\n","Saving index to: ./hnswlib.index\n"]}],"source":["#Defining our hnswlib index\n","index_path = \"./hnswlib.index\"\n","#We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n","import hnswlib\n","import numpy as np\n","index = hnswlib.Index(space = 'cosine', dim = embedding_size)\n","\n","if os.path.exists(index_path):\n","    print(\"Loading index...\")\n","    index.load_index(index_path)\n","    print('Loading index done.')\n","else:\n","    ### Create the HNSWLIB index\n","    print(\"Start creating HNSWLIB index\")\n","    # TODO check is ef_consturction and M and appropriate for this dataset\n","    index.init_index(max_elements = len(corpus_embeddings), ef_construction = 400, M = 64)\n","\n","    print(list(range(len(corpus_embeddings))))\n","    # Then we train the index to find a suitable clustering\n","    corpus_embeddings = np.array(corpus_embeddings).squeeze()\n","    print('shape', corpus_embeddings.shape)\n","    index.add_items(corpus_embeddings, list(range(len(corpus_embeddings))))\n","\n","    print(\"Saving index to:\", index_path)\n","    index.save_index(index_path)\n","\n","# Controlling the recall by setting ef:\n","index.set_ef(50)  # ef should always be > top_k_hits\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus loaded with 149 sentences\n"]}],"source":["######### Search in the index ###########\n","\n","print(f\"Corpus loaded with {len(corpus_sentences)} sentences\")\n","\n","\n","# inp_question = input(\"Enter your question: \")\n","inp_question = \"Why is my dishwasher leaking?\"\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["length of question embedding 768\n","hits [{'corpus_id': 55, 'score': 0.7006855607032776}, {'corpus_id': 20, 'score': 0.5618450045585632}, {'corpus_id': 56, 'score': 0.5229777693748474}, {'corpus_id': 40, 'score': 0.507260799407959}, {'corpus_id': 1, 'score': 0.505429208278656}]\n","Input question: Why is my dishwasher leaking?\n","Results (after 0.026 seconds):\n","type <class 'pandas.core.frame.DataFrame'>\n","\t0.701\tcontent     There are two general areas where you will co...\n","tokens                                                    77\n","Name: (Chapter_4, 190), dtype: object\n","\t0.562\tcontent     POOR WASH QUALITY: This is the most common co...\n","tokens                                                    77\n","Name: (Chapter_2, 81), dtype: object\n","\t0.523\tcontent     A slow, under-tub leak may go years without b...\n","tokens                                                    71\n","Name: (Chapter_4, 191), dtype: object\n","\t0.507\tcontent     NOTE: This Chapter assumes that the motor is ...\n","tokens                                                   124\n","Name: (Chapter_3, 145), dtype: object\n","\t0.505\tcontent     The main reason dishwashers exist is that the...\n","tokens                                                    76\n","Name: (Chapter_1, 2), dtype: object\n"]}],"source":["start_time = time.time()\n","question_embedding = embeddings_model.encode(inp_question)\n","print('length of question embedding', len(question_embedding))\n","\n","#We use hnswlib knn_query method to find the top_k_hits\n","corpus_ids, distances = index.knn_query(question_embedding, k=top_k_hits)\n","# print('corpus_ids', corpus_ids)\n","\n","# We extract corpus ids and scores for the first query\n","hits = [{'corpus_id': id, 'score': 1 - score} for id, score in zip(corpus_ids[0], distances[0])]\n","hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n","print('hits', hits)\n","end_time = time.time()\n","\n","print(\"Input question:\", inp_question)\n","print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n","print('type', type(corpus_sentences))\n","# print(corpus_sentences)\n","for hit in hits[0:top_k_hits]:\n","    print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences.iloc[hit['corpus_id']]))\n","\n","# # Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n","# # Here, we compute the recall of ANN compared to the exact results\n","# correct_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k_hits)[0]\n","# correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n","\n","# ann_corpus_ids = set([hit['corpus_id'] for hit in hits])\n","# if len(ann_corpus_ids) != len(correct_hits_ids):\n","#     print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n","\n","# recall = len(ann_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n","# print(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n","\n","# if recall < 1:\n","#     print(\"Missing results:\")\n","#     for hit in correct_hits[0:top_k_hits]:\n","#         if hit['corpus_id'] not in ann_corpus_ids:\n","#             print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n","# print(\"\\n\\n========\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('openai')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3c5ba64e8a2a59a6be0dd5b8a0149005a96dade111bc9e6b7c5d65266a44d405"}}},"nbformat":4,"nbformat_minor":2}
